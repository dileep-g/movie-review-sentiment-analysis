{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rK4lV6HSSdeI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import numpy as np\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3O97VvUSdeN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJhNTAdbSdeP",
    "outputId": "04195b5e-02ea-4dca-9c42-c0d6710757a5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7bLl97bSdeh"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/trs_train.tsv', sep='\\t')\n",
    "X_train = (df['sentence'])\n",
    "y_train = df['label']\n",
    "df_test = pd.read_csv('data/trs_dev.tsv', sep='\\t')\n",
    "X_test = df_test['sentence']\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Train:\n",
      "progress: ########################################"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-56f5af0dd94b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtt_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Transform Train:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mreplace_with_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train len: {tr_len}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-56f5af0dd94b>\u001b[0m in \u001b[0;36mreplace_with_not\u001b[0;34m(X, A)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_nt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_not\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs670/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs670/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;31m# check for chained assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;31m# actually do the set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs670/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                 self._check_setitem_copy(stacklevel=4, t='referant',\n\u001b[0;32m-> 2627\u001b[0;31m                                          force=True)\n\u001b[0m\u001b[1;32m   2628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs670/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[0;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[1;32m   2671\u001b[0m             \u001b[0;31m# the copy weakref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2674\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "\n",
    "def replace_with_not(X, A):\n",
    "    w_nt = \"n't\"\n",
    "    w_not = \"not\"\n",
    "    p = 0\n",
    "    print('progress: ', end='')\n",
    "    for i in range(len(X)):\n",
    "        sent = X_train.loc[i]\n",
    "        sent = sent.replace(w_nt, w_not)\n",
    "        X_train.loc[i] = lemmatize(sent, A)\n",
    "        if p%500 == 0:\n",
    "            print('#', end='')\n",
    "        p += 1\n",
    "\n",
    "def lemmatize(s, A):\n",
    "    sentence = TextBlob(s)\n",
    "#     print(f'len: {len(sentence.words)}, words: {sentence.words}')\n",
    "    words = []\n",
    "    for i in range(len(sentence.words)):\n",
    "        words.append(Word(sentence.words[i].lemmatize()))\n",
    "    A.append(len(words))\n",
    "    return \" \".join(words)\n",
    "    \n",
    "    \n",
    "tr_len = []\n",
    "tt_len = []\n",
    "print('Transform Train:')\n",
    "replace_with_not(X_train, tr_len)\n",
    "print(f'train len: {tr_len}')\n",
    "\n",
    "print('Transform Test:')\n",
    "replace_with_not(X_test, tt_len)\n",
    "print(f'test len: {tt_len}')\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQbGLnjvSdfK"
   },
   "outputs": [],
   "source": [
    "dimensions = 300\n",
    "# # Get embeddings from fast text file\n",
    "pretrained_vectors = {}\n",
    "f = open('wiki-news-300d-1M.vec', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.rstrip().rsplit(' ', dimensions)\n",
    "    word = values[0]\n",
    "    probabilities = np.asarray(values[1:], dtype='float32')\n",
    "    pretrained_vectors[word] = probabilities\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yCD-9QApSdfR",
    "outputId": "ec47b692-c0a3-4c45-cc4a-2b03dc2480f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.640e-02  1.014e-01 -7.890e-02 -8.490e-02 -4.530e-02 -3.090e-02\n",
      "  9.820e-02  5.180e-02 -2.120e-02  1.377e-01  9.450e-02 -6.990e-02\n",
      " -6.100e-02 -6.440e-02  9.630e-02  1.563e-01 -1.418e-01 -3.340e-02\n",
      "  9.620e-02  3.670e-02 -2.901e-01  1.070e-02 -1.039e-01  5.210e-02\n",
      "  3.760e-02 -9.350e-02  1.076e-01  1.002e-01 -4.250e-02  4.830e-02\n",
      "  1.926e-01 -1.000e-03  7.610e-02  6.600e-02  3.150e-02  1.808e-01\n",
      " -8.100e-02 -6.000e-02  1.870e-02 -6.240e-02  8.000e-04  1.161e-01\n",
      " -3.350e-02 -2.510e-02  6.240e-02  3.260e-02 -3.490e-02 -8.330e-02\n",
      "  1.470e-02 -1.384e-01  2.293e-01 -1.290e-02 -6.591e-01  1.712e-01\n",
      " -5.610e-02  1.558e-01 -8.000e-02  9.910e-02 -1.858e-01 -8.670e-02\n",
      "  1.081e-01  1.200e-02  2.130e-02  5.000e-02  4.600e-02  4.140e-02\n",
      " -5.770e-02  1.147e-01 -5.440e-02 -2.680e-02  9.800e-03  5.390e-02\n",
      " -8.700e-02 -6.130e-02 -2.610e-02  1.290e-02 -4.790e-02 -4.500e-02\n",
      "  5.410e-02 -1.081e-01  2.600e-03 -1.019e-01 -6.060e-02 -2.171e-01\n",
      "  4.550e-02  4.230e-02  1.220e-02 -1.551e-01  2.341e-01  8.940e-02\n",
      "  9.300e-03 -1.460e-01 -9.870e-02 -5.890e-02 -7.410e-02 -1.997e-01\n",
      " -7.440e-02  1.863e-01  5.260e-02  2.259e-01 -1.967e-01 -1.160e-01\n",
      " -2.980e-02  1.690e-02 -5.640e-02  1.500e-02 -9.890e-02 -4.300e-03\n",
      "  1.060e-02  3.900e-03  8.060e-02  5.820e-02  1.369e-01 -1.061e-01\n",
      " -1.567e-01  4.190e-02 -2.920e-02  4.450e-02 -4.700e-03 -3.753e-01\n",
      " -3.180e-02  7.640e-02 -5.450e-02  1.220e-02  5.710e-02  1.959e-01\n",
      " -1.669e-01 -9.720e-02 -3.060e-02  2.550e-02  2.520e-02  2.410e-02\n",
      " -7.470e-02  1.121e-01 -8.860e-02  8.410e-02 -8.190e-02  2.550e-02\n",
      "  8.200e-03 -1.028e-01 -2.790e-02 -1.360e-02 -1.817e-01  1.026e-01\n",
      "  9.810e-02 -5.920e-02 -4.000e-04 -1.100e-01 -3.070e-02  4.290e-02\n",
      " -4.390e-02  2.100e-03  9.070e-02  2.680e-02  6.160e-02  1.661e-01\n",
      " -8.680e-02  6.330e-02 -3.640e-02 -7.670e-02 -1.630e-02 -1.882e-01\n",
      "  1.339e-01  1.368e-01  3.700e-03  1.618e-01  8.080e-02 -5.290e-02\n",
      " -1.022e-01 -9.950e-02 -2.070e-02 -8.610e-02 -3.840e-02  1.591e-01\n",
      "  8.430e-02 -6.460e-02  2.953e-01 -1.402e-01  4.800e-02 -1.458e-01\n",
      "  8.030e-02 -8.400e-03 -9.640e-02 -5.590e-02  7.950e-02 -9.810e-02\n",
      "  2.480e-02  3.720e-02 -2.230e-01  7.360e-02 -3.620e-02  5.460e-02\n",
      " -1.355e-01  8.630e-02  4.330e-02  3.315e-01  1.061e-01  2.580e-02\n",
      "  6.640e-02 -1.316e-01 -1.732e-01 -6.140e-02  1.024e-01 -9.590e-02\n",
      "  1.500e-02 -1.440e-01  7.640e-02 -1.089e-01 -1.510e-02  1.970e-02\n",
      "  1.460e-01  9.710e-02  9.130e-02 -5.900e-03  8.350e-02  1.067e-01\n",
      " -1.050e-01  9.900e-02 -1.802e-01  6.870e-02  2.970e-02 -1.682e-01\n",
      "  1.400e-02 -4.350e-02  9.300e-02 -8.560e-02  5.920e-02 -3.950e-02\n",
      "  5.600e-02 -1.964e-01 -7.440e-02 -7.490e-02  3.253e-01 -1.407e-01\n",
      " -1.196e-01 -2.030e-02  1.210e-02  1.452e-01 -2.613e-01  8.770e-02\n",
      " -8.860e-02 -5.970e-02  9.500e-03  1.681e-01  1.354e-01 -7.800e-02\n",
      "  8.350e-02 -4.050e-02 -1.027e-01  3.718e-01  7.310e-02 -9.160e-02\n",
      " -5.700e-03 -2.164e-01 -1.392e-01  1.208e-01 -7.090e-02  1.150e-01\n",
      " -4.220e-02  2.228e-01  1.150e-02  2.610e-02 -8.960e-02  2.280e-02\n",
      " -3.974e-01  3.140e-02 -1.801e-01  2.520e-02 -1.345e-01 -6.860e-02\n",
      " -1.043e-01  5.950e-02 -3.960e-02  2.290e-02 -1.691e-01  1.256e-01\n",
      "  5.810e-02 -8.770e-02 -1.162e-01  9.980e-02  1.127e-01  1.362e-01\n",
      "  3.180e-02 -1.078e-01  8.720e-02 -5.460e-02 -5.840e-02  2.090e-02\n",
      "  1.342e-01  6.030e-02  5.400e-02 -1.708e-01  8.060e-02 -1.057e-01\n",
      " -1.640e-02 -4.070e-02 -5.260e-02  4.710e-02 -6.980e-02 -1.790e-02]\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_vectors['movie'])\n",
    "# print(pretrained_vectors[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THm98g7rSdfY"
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from keras import constraints\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXqHtFd3Sdfg"
   },
   "outputs": [],
   "source": [
    "tknzr = Tokenizer(char_level=False, \n",
    "                  oov_token=None, \n",
    "                  num_words=None, \n",
    "                  split=' ', \n",
    "                  lower=True, \n",
    "                  filters='!\"#$%&()*+,\\t-./:;\\n<=>?@[\\]^_`{|}~ ', \n",
    "                  document_count=0)\n",
    "\n",
    "list_x_train = list(X_train)\n",
    "tknzr.fit_on_texts(list_x_train)\n",
    "\n",
    "train_seq = tknzr.texts_to_sequences(X_train)\n",
    "test_seq = tknzr.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVDnT3JsSdfn"
   },
   "outputs": [],
   "source": [
    "num_words = len(tknzr.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAgE_nl4Sdfu",
    "outputId": "45c9923b-192b-475e-f0c0-20c3fd968da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13823\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hs89586kSdf2",
    "outputId": "ad604582-6447-4559-b72f-073c3374d519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(max(len(sequence) for sequence in train_seq))\n",
    "processing_length = int(np.percentile([len(sequence) for sequence in train_seq], 98))\n",
    "print(processing_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bpc0vlrrSdf6"
   },
   "outputs": [],
   "source": [
    "padded_X_train = pad_sequences(train_seq, maxlen=processing_length, padding='post', value=0.0, truncating='post')\n",
    "padded_X_test = pad_sequences(test_seq, maxlen=processing_length, padding='post', value=0.0, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNy7pBD6Sdf9",
    "outputId": "5238fd2b-d8e2-4b09-d518-988808972aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment:  rich veins of funny stuff in this movie \n",
      "Tokenized Comment: [379, 9656, 4, 49, 409, 9, 18, 17]\n"
     ]
    }
   ],
   "source": [
    "for index, (token, token_index) in enumerate(zip(X_train[60:61], train_seq[60:61])):\n",
    "    print('Comment:  {}'.format(token))\n",
    "    print('Tokenized Comment: {}'.format(token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OCQmnXaSdgB",
    "outputId": "d18a57fb-1bd7-4ec4-cc4c-057403c6144d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10    6    2 ...    0    0    0]\n",
      " [3143    3 1738 ...    0    0    0]\n",
      " [ 863   89    5 ...    0    0    0]\n",
      " ...\n",
      " [   2  295   34 ...    0    0    0]\n",
      " [   7   13  531 ...    0    0    0]\n",
      " [ 313 6556  235 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGqJ71x3SdgG"
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix():\n",
    "    embed_matrix = np.zeros((len(tknzr.word_index) + 1, dimensions))\n",
    "    for word, index in tknzr.word_index.items():\n",
    "        vector = pretrained_vectors.get(word)\n",
    "        if vector is not None:\n",
    "            embed_matrix[index] = vector\n",
    "    return embed_matrix\n",
    "\n",
    "embd_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei4w1qfNSdgK",
    "outputId": "bf1abdaa-32ac-487f-e975-e0898d53c863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.640e-02  1.014e-01 -7.890e-02 -8.490e-02 -4.530e-02 -3.090e-02\n",
      "  9.820e-02  5.180e-02 -2.120e-02  1.377e-01  9.450e-02 -6.990e-02\n",
      " -6.100e-02 -6.440e-02  9.630e-02  1.563e-01 -1.418e-01 -3.340e-02\n",
      "  9.620e-02  3.670e-02 -2.901e-01  1.070e-02 -1.039e-01  5.210e-02\n",
      "  3.760e-02 -9.350e-02  1.076e-01  1.002e-01 -4.250e-02  4.830e-02\n",
      "  1.926e-01 -1.000e-03  7.610e-02  6.600e-02  3.150e-02  1.808e-01\n",
      " -8.100e-02 -6.000e-02  1.870e-02 -6.240e-02  8.000e-04  1.161e-01\n",
      " -3.350e-02 -2.510e-02  6.240e-02  3.260e-02 -3.490e-02 -8.330e-02\n",
      "  1.470e-02 -1.384e-01  2.293e-01 -1.290e-02 -6.591e-01  1.712e-01\n",
      " -5.610e-02  1.558e-01 -8.000e-02  9.910e-02 -1.858e-01 -8.670e-02\n",
      "  1.081e-01  1.200e-02  2.130e-02  5.000e-02  4.600e-02  4.140e-02\n",
      " -5.770e-02  1.147e-01 -5.440e-02 -2.680e-02  9.800e-03  5.390e-02\n",
      " -8.700e-02 -6.130e-02 -2.610e-02  1.290e-02 -4.790e-02 -4.500e-02\n",
      "  5.410e-02 -1.081e-01  2.600e-03 -1.019e-01 -6.060e-02 -2.171e-01\n",
      "  4.550e-02  4.230e-02  1.220e-02 -1.551e-01  2.341e-01  8.940e-02\n",
      "  9.300e-03 -1.460e-01 -9.870e-02 -5.890e-02 -7.410e-02 -1.997e-01\n",
      " -7.440e-02  1.863e-01  5.260e-02  2.259e-01 -1.967e-01 -1.160e-01\n",
      " -2.980e-02  1.690e-02 -5.640e-02  1.500e-02 -9.890e-02 -4.300e-03\n",
      "  1.060e-02  3.900e-03  8.060e-02  5.820e-02  1.369e-01 -1.061e-01\n",
      " -1.567e-01  4.190e-02 -2.920e-02  4.450e-02 -4.700e-03 -3.753e-01\n",
      " -3.180e-02  7.640e-02 -5.450e-02  1.220e-02  5.710e-02  1.959e-01\n",
      " -1.669e-01 -9.720e-02 -3.060e-02  2.550e-02  2.520e-02  2.410e-02\n",
      " -7.470e-02  1.121e-01 -8.860e-02  8.410e-02 -8.190e-02  2.550e-02\n",
      "  8.200e-03 -1.028e-01 -2.790e-02 -1.360e-02 -1.817e-01  1.026e-01\n",
      "  9.810e-02 -5.920e-02 -4.000e-04 -1.100e-01 -3.070e-02  4.290e-02\n",
      " -4.390e-02  2.100e-03  9.070e-02  2.680e-02  6.160e-02  1.661e-01\n",
      " -8.680e-02  6.330e-02 -3.640e-02 -7.670e-02 -1.630e-02 -1.882e-01\n",
      "  1.339e-01  1.368e-01  3.700e-03  1.618e-01  8.080e-02 -5.290e-02\n",
      " -1.022e-01 -9.950e-02 -2.070e-02 -8.610e-02 -3.840e-02  1.591e-01\n",
      "  8.430e-02 -6.460e-02  2.953e-01 -1.402e-01  4.800e-02 -1.458e-01\n",
      "  8.030e-02 -8.400e-03 -9.640e-02 -5.590e-02  7.950e-02 -9.810e-02\n",
      "  2.480e-02  3.720e-02 -2.230e-01  7.360e-02 -3.620e-02  5.460e-02\n",
      " -1.355e-01  8.630e-02  4.330e-02  3.315e-01  1.061e-01  2.580e-02\n",
      "  6.640e-02 -1.316e-01 -1.732e-01 -6.140e-02  1.024e-01 -9.590e-02\n",
      "  1.500e-02 -1.440e-01  7.640e-02 -1.089e-01 -1.510e-02  1.970e-02\n",
      "  1.460e-01  9.710e-02  9.130e-02 -5.900e-03  8.350e-02  1.067e-01\n",
      " -1.050e-01  9.900e-02 -1.802e-01  6.870e-02  2.970e-02 -1.682e-01\n",
      "  1.400e-02 -4.350e-02  9.300e-02 -8.560e-02  5.920e-02 -3.950e-02\n",
      "  5.600e-02 -1.964e-01 -7.440e-02 -7.490e-02  3.253e-01 -1.407e-01\n",
      " -1.196e-01 -2.030e-02  1.210e-02  1.452e-01 -2.613e-01  8.770e-02\n",
      " -8.860e-02 -5.970e-02  9.500e-03  1.681e-01  1.354e-01 -7.800e-02\n",
      "  8.350e-02 -4.050e-02 -1.027e-01  3.718e-01  7.310e-02 -9.160e-02\n",
      " -5.700e-03 -2.164e-01 -1.392e-01  1.208e-01 -7.090e-02  1.150e-01\n",
      " -4.220e-02  2.228e-01  1.150e-02  2.610e-02 -8.960e-02  2.280e-02\n",
      " -3.974e-01  3.140e-02 -1.801e-01  2.520e-02 -1.345e-01 -6.860e-02\n",
      " -1.043e-01  5.950e-02 -3.960e-02  2.290e-02 -1.691e-01  1.256e-01\n",
      "  5.810e-02 -8.770e-02 -1.162e-01  9.980e-02  1.127e-01  1.362e-01\n",
      "  3.180e-02 -1.078e-01  8.720e-02 -5.460e-02 -5.840e-02  2.090e-02\n",
      "  1.342e-01  6.030e-02  5.400e-02 -1.708e-01  8.060e-02 -1.057e-01\n",
      " -1.640e-02 -4.070e-02 -5.260e-02  4.710e-02 -6.980e-02 -1.790e-02]\n",
      "##############################################\n",
      "[-3.64000015e-02  1.01400003e-01 -7.89000019e-02 -8.48999992e-02\n",
      " -4.52999994e-02 -3.08999997e-02  9.82000008e-02  5.18000014e-02\n",
      " -2.11999994e-02  1.37700006e-01  9.44999978e-02 -6.98999986e-02\n",
      " -6.10000007e-02 -6.44000024e-02  9.62999985e-02  1.56299993e-01\n",
      " -1.41800001e-01 -3.33999991e-02  9.61999968e-02  3.66999991e-02\n",
      " -2.90100008e-01  1.07000005e-02 -1.03900000e-01  5.20999990e-02\n",
      "  3.75999995e-02 -9.35000032e-02  1.07600003e-01  1.00199997e-01\n",
      " -4.25000004e-02  4.83000018e-02  1.92599997e-01 -1.00000005e-03\n",
      "  7.60999992e-02  6.59999996e-02  3.15000005e-02  1.80800006e-01\n",
      " -8.10000002e-02 -5.99999987e-02  1.86999999e-02 -6.23999983e-02\n",
      "  7.99999980e-04  1.16099998e-01 -3.35000008e-02 -2.51000002e-02\n",
      "  6.23999983e-02  3.26000005e-02 -3.48999985e-02 -8.33000019e-02\n",
      "  1.47000002e-02 -1.38400003e-01  2.29300007e-01 -1.29000004e-02\n",
      " -6.59099996e-01  1.71200007e-01 -5.60999997e-02  1.55800000e-01\n",
      " -7.99999982e-02  9.91000012e-02 -1.85800001e-01 -8.66999999e-02\n",
      "  1.08099997e-01  1.20000001e-02  2.12999992e-02  5.00000007e-02\n",
      "  4.60000001e-02  4.14000005e-02 -5.77000007e-02  1.14699997e-01\n",
      " -5.44000007e-02 -2.67999992e-02  9.80000012e-03  5.38999997e-02\n",
      " -8.69999975e-02 -6.12999983e-02 -2.61000004e-02  1.29000004e-02\n",
      " -4.78999987e-02 -4.50000018e-02  5.40999994e-02 -1.08099997e-01\n",
      "  2.60000001e-03 -1.01899996e-01 -6.06000014e-02 -2.17099994e-01\n",
      "  4.54999991e-02  4.23000008e-02  1.21999998e-02 -1.55100003e-01\n",
      "  2.34099999e-01  8.94000009e-02  9.30000003e-03 -1.45999998e-01\n",
      " -9.87000018e-02 -5.88999987e-02 -7.41000026e-02 -1.99699998e-01\n",
      " -7.44000003e-02  1.86299995e-01  5.26000001e-02  2.25899994e-01\n",
      " -1.96700007e-01 -1.15999997e-01 -2.97999997e-02  1.68999992e-02\n",
      " -5.64000010e-02  1.49999997e-02 -9.88999978e-02 -4.30000015e-03\n",
      "  1.05999997e-02  3.89999989e-03  8.06000009e-02  5.82000017e-02\n",
      "  1.36899993e-01 -1.06100000e-01 -1.56700000e-01  4.19000015e-02\n",
      " -2.92000007e-02  4.45000008e-02 -4.69999993e-03 -3.75299990e-01\n",
      " -3.18000019e-02  7.63999969e-02 -5.44999987e-02  1.21999998e-02\n",
      "  5.71000017e-02  1.95899993e-01 -1.66899994e-01 -9.71999988e-02\n",
      " -3.06000002e-02  2.54999995e-02  2.52000000e-02  2.41000000e-02\n",
      " -7.46999979e-02  1.12099998e-01 -8.86000022e-02  8.41000006e-02\n",
      " -8.19000006e-02  2.54999995e-02  8.20000004e-03 -1.02799997e-01\n",
      " -2.78999992e-02 -1.36000002e-02 -1.81700006e-01  1.02600001e-01\n",
      "  9.80999991e-02 -5.92000000e-02 -3.99999990e-04 -1.09999999e-01\n",
      " -3.07000000e-02  4.28999998e-02 -4.39000018e-02  2.09999993e-03\n",
      "  9.07000005e-02  2.67999992e-02  6.15999997e-02  1.66099995e-01\n",
      " -8.68000016e-02  6.32999986e-02 -3.64000015e-02 -7.67000020e-02\n",
      " -1.63000003e-02 -1.88199997e-01  1.33900002e-01  1.36800006e-01\n",
      "  3.70000000e-03  1.61799997e-01  8.07999969e-02 -5.29000014e-02\n",
      " -1.02200001e-01 -9.95000005e-02 -2.07000002e-02 -8.60999972e-02\n",
      " -3.84000018e-02  1.59099996e-01  8.42999965e-02 -6.45999983e-02\n",
      "  2.95300007e-01 -1.40200004e-01  4.80000004e-02 -1.45799994e-01\n",
      "  8.03000033e-02 -8.39999970e-03 -9.64000002e-02 -5.59000000e-02\n",
      "  7.94999972e-02 -9.80999991e-02  2.48000007e-02  3.72000001e-02\n",
      " -2.23000005e-01  7.36000016e-02 -3.62000018e-02  5.46000004e-02\n",
      " -1.35499999e-01  8.63000005e-02  4.32999991e-02  3.31499994e-01\n",
      "  1.06100000e-01  2.58000009e-02  6.63999990e-02 -1.31600007e-01\n",
      " -1.73199996e-01 -6.14000000e-02  1.02399997e-01 -9.58999991e-02\n",
      "  1.49999997e-02 -1.43999994e-01  7.63999969e-02 -1.08900003e-01\n",
      " -1.51000004e-02  1.97000001e-02  1.45999998e-01  9.70999971e-02\n",
      "  9.13000032e-02 -5.90000022e-03  8.34999979e-02  1.06700003e-01\n",
      " -1.04999997e-01  9.89999995e-02 -1.80199996e-01  6.87000006e-02\n",
      "  2.96999998e-02 -1.68200001e-01  1.40000004e-02 -4.34999987e-02\n",
      "  9.30000022e-02 -8.56000036e-02  5.92000000e-02 -3.95000018e-02\n",
      "  5.60000017e-02 -1.96400002e-01 -7.44000003e-02 -7.49000013e-02\n",
      "  3.25300008e-01 -1.40699998e-01 -1.19599998e-01 -2.03000009e-02\n",
      "  1.20999999e-02  1.45199999e-01 -2.61299998e-01  8.77000019e-02\n",
      " -8.86000022e-02 -5.97000010e-02  9.49999969e-03  1.68099999e-01\n",
      "  1.35399997e-01 -7.80000016e-02  8.34999979e-02 -4.05000001e-02\n",
      " -1.02700002e-01  3.71800005e-01  7.31000006e-02 -9.16000009e-02\n",
      " -5.70000010e-03 -2.16399997e-01 -1.39200002e-01  1.20800003e-01\n",
      " -7.09000006e-02  1.15000002e-01 -4.21999991e-02  2.22800002e-01\n",
      "  1.15000000e-02  2.61000004e-02 -8.95999968e-02  2.28000004e-02\n",
      " -3.97399992e-01  3.13999988e-02 -1.80099994e-01  2.52000000e-02\n",
      " -1.34499997e-01 -6.85999990e-02 -1.04300000e-01  5.95000014e-02\n",
      " -3.95999998e-02  2.29000002e-02 -1.69100001e-01  1.25599995e-01\n",
      "  5.81000000e-02 -8.77000019e-02 -1.16200000e-01  9.97999981e-02\n",
      "  1.12700000e-01  1.36199996e-01  3.18000019e-02 -1.07799999e-01\n",
      "  8.72000009e-02 -5.46000004e-02 -5.84000014e-02  2.08999999e-02\n",
      "  1.34200007e-01  6.03000000e-02  5.40000014e-02 -1.70800000e-01\n",
      "  8.06000009e-02 -1.05700001e-01 -1.64000001e-02 -4.06999998e-02\n",
      " -5.26000001e-02  4.71000001e-02 -6.97999969e-02 -1.78999994e-02]\n"
     ]
    }
   ],
   "source": [
    "index = tknzr.word_index['movie']\n",
    "print(pretrained_vectors['movie']) \n",
    "print(embd_matrix[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqifHtL0SdgT",
    "outputId": "c0abd81d-839c-4906-9dd5-3ff17cf71f06",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 300)           4146900   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 60)            86640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 60)            240       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 7, 60)             0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 4,237,081\n",
      "Trainable params: 4,236,861\n",
      "Non-trainable params: 220\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(len(tknzr.word_index) + 1, dimensions, weights=[embd_matrix], input_length=processing_length, trainable=True))\n",
    "lstm_model.add(LSTM(60, return_sequences=True))\n",
    "lstm_model.add(BatchNormalization())\n",
    "lstm_model.add(MaxPooling1D(4))\n",
    "lstm_model.add(GlobalMaxPooling1D())\n",
    "lstm_model.add(layers.Dense(50, activation='relu'))\n",
    "lstm_model.add(layers.BatchNormalization())\n",
    "lstm_model.add(layers.Dropout(0.3))\n",
    "lstm_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbAmgpoGSdgY",
    "outputId": "ac8ee55c-1d64-4952-9021-60e19a3e22f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 32)            442336    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 495,637\n",
      "Trainable params: 495,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# embedding_size=32\n",
    "# model=Sequential()\n",
    "# model.add(Embedding(num_words, embedding_size, input_length=processing_length))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrwMlwKKSdgi"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2UHXDShSdgq",
    "outputId": "bb05c7ca-66e6-40dc-a8e5-6d1f6bfbf6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "67349/67349 [==============================] - 35s 519us/step - loss: 0.0131 - acc: 0.9936\n",
      "Epoch 2/3\n",
      "67349/67349 [==============================] - 35s 521us/step - loss: 0.0118 - acc: 0.9937\n",
      "Epoch 3/3\n",
      "67349/67349 [==============================] - 35s 516us/step - loss: 0.0107 - acc: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dde837ddd8>"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "model.fit(padded_X_train, y_train, batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1fKpmfwSdg0",
    "outputId": "2cd47d09-1f61-46f5-d45a-79ff46c61adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: [1.036240967862103, 0.8084862385321101]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(padded_X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CVBkXCqSdg4",
    "outputId": "673ead57-cd2e-4090-d0fd-d00022949218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train vec: (67349, 14296)\n",
      "shape of test vec: (872, 14296)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "movie_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)\n",
    "# movie_counts = movie_vec.fit_transform()\n",
    "nb_train_vec = movie_vec.fit_transform(X_train)\n",
    "nb_test_vec = movie_vec.transform(X_test)\n",
    "print(f'shape of train vec: {nb_train_vec.shape}')\n",
    "print(f'shape of test vec: {nb_test_vec.shape}')\n",
    "\n",
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(nb_train_vec)\n",
    "test_tfidf = tfidf_transformer.transform(nb_test_vec)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwfDC-tSSdg7",
    "outputId": "1b01c837-2842-4214-f7c4-a4a57bd052a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (67349, 14296), test:(872, 14296)\n",
      "Accuracy for Naives Bayes: 0.8130733944954128\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = BernoulliNB().fit(train_tfidf, y_train)\n",
    "\n",
    "print(f'shape of train: {train_tfidf.shape}, test:{test_tfidf.shape}')\n",
    "# Predicting the Test set results, find accuracy\n",
    "y_pred = clf.predict(test_tfidf)\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy for Naives Bayes: {acc}')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out different Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: hide new secretions from the parental units\n",
      "1: contains no wit , only labored gags\n",
      "3: remains utterly satisfied to remain the same throughout\n",
      "4: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up\n",
      "5: that 's far too tragic to merit such superficial treatment\n",
      "8: a depressed fifteen-year-old 's suicidal poetry\n",
      "10: goes to absurd lengths\n",
      "11: for those moviegoers who complain that ` they do not make movies like they used to anymore\n",
      "12: the part where nothing 's happening ,\n",
      "13: saw how bad this movie was\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "for i in range(y_train.shape[0]):\n",
    "    if y_train[i]==0:\n",
    "        print(f'{i}: {X_train[i]}')\n",
    "#         print(f'sentiment: {TextBlob(X_train[i]).sentiment}')\n",
    "        t+=1\n",
    "    if t==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 1, words: ['no']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "str = \"no\"\n",
    "sentence = TextBlob(str)\n",
    "print(f'len: {len(sentence.words)}, words: {sentence.words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those moviegoers who complain that ` they do n't make movies like they used to anymore \n",
      "for those moviegoers who complain that ` they do not make movies like they used to anymore \n",
      "for those moviegoers who complain that ` they do n't make movies like they used to anymore \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading https://files.pythonhosted.org/packages/67/f7/1462c6d28ec27ef2812aa2e9376c7fc7b39a23f0e02297f71119d74375c5/contractions-0.0.18-py2.py3-none-any.whl\n",
      "Installing collected packages: contractions\n",
      "Successfully installed contractions-0.0.18\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install textblob\n",
    "# !{sys.executable} -m pip install jamspell\n",
    "# !conda install --yes --prefix {sys.prefix} swig\n",
    "# !{sys.executable} -m pip install pycontractions\n",
    "# !{sys.executable} -m pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from pycontractions import Contractions\n",
    "\n",
    "cont = Contractions(api_key=\"glove-twitter-100\")\n",
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dgunda/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment contains no wit: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "TextBlob correction contains no wit: contains no wit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sent = \"contains no wit\"\n",
    "print(f'sentiment {sent}: {TextBlob(sent).sentiment}')\n",
    "print(f'TextBlob correction {sent}: {TextBlob(sent).correct()}')\n",
    "\n",
    "# crct_sent = list(cont.expand_texts([sent]))\n",
    "# print(f'pycontra {sent}: {crct_sent}')\n",
    "# print(f'TextBlob correction {sent}: {TextBlob(sent).correct()}')\n",
    "\n",
    "# import contractions\n",
    "# print(f'crct: {contractions.fix(sent)}')\n",
    "\n",
    "# # First, you're going to need to import wordnet: \n",
    "# from nltk.corpus import wordnet \n",
    "  \n",
    "# # Then, we're going to use the term \"program\" to find synsets like so: \n",
    "# syns = wordnet.synsets(sent) \n",
    "# print(syns[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of nb_train_vec: (67349, 14297)\n",
      "hide new secretions from the parental units \n",
      "count vector: \n",
      "  (0, 5086)\t1\n",
      "  (0, 5900)\t1\n",
      "  (0, 8440)\t1\n",
      "  (0, 9019)\t1\n",
      "  (0, 10988)\t1\n",
      "  (0, 12670)\t1\n",
      "  (0, 13407)\t1\n",
      "tfidf vector: \n",
      "  (0, 13407)\t0.4725778010918928\n",
      "  (0, 12670)\t0.10311924385970848\n",
      "  (0, 10988)\t0.49857746188436197\n",
      "  (0, 9019)\t0.46090177259444287\n",
      "  (0, 8440)\t0.2603119592841447\n",
      "  (0, 5900)\t0.4371688950946575\n",
      "  (0, 5086)\t0.21482028534700234\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_Project_Final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
